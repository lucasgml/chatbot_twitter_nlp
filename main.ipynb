{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telepot\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "import tweepy as tw\n",
    "from telepot.loop import MessageLoop\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from nltk import sent_tokenize, TweetTokenizer, WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import gensim.downloader as down\n",
    "from gensim.corpora import Dictionary\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot:\n",
    "    def __init__(self):\n",
    "        print(\"Preparing bot...\")\n",
    "        with open('bot.json','r') as file:\n",
    "            bot_api = file.read()\n",
    "            json_object = json.loads(bot_api)\n",
    "            self.bot = telepot.Bot(json_object['api'])\n",
    "            self.nlp = spacy.load('en')\n",
    "        print(\"Done!\")\n",
    "        \n",
    "    def strip_accents(self,text):\n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError: # unicode is a default on python 3 \n",
    "            pass\n",
    "\n",
    "        text = unicodedata.normalize('NFD', text)\\\n",
    "               .encode('ascii', 'ignore')\\\n",
    "               .decode(\"utf-8\")\n",
    "\n",
    "        return str(text)\n",
    "        \n",
    "    def find_entities(self,text):\n",
    "        entities = []\n",
    "        doc = self.nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            entities.append([ent.ents[0], ent.ents[0].label_])\n",
    "        return entities\n",
    "    \n",
    "    def entity_type(self,ents,label):\n",
    "        return([str(ent[0]) for ent in ents if ent[1] == label])\n",
    "    \n",
    "    def prepare_query(self,ents):\n",
    "        string = '('\n",
    "        size = len(ents)\n",
    "        i = 0\n",
    "        list_of_words = []\n",
    "        for ent in ents:\n",
    "            words = ent.split(' ')\n",
    "            for word in words:\n",
    "                list_of_words.append(word)\n",
    "        while(i < size):\n",
    "            string += list_of_words[i]\n",
    "            if(i != size - 1):\n",
    "                string += ' AND ' #' OR '\n",
    "            else:\n",
    "                string += ')'\n",
    "            i += 1\n",
    "        return string\n",
    "    \n",
    "    def cleaning_tweets(self,tweets):\n",
    "        list_of_words = list()\n",
    "        lemma = WordNetLemmatizer()\n",
    "        tweet_token = TweetTokenizer(strip_handles=True)\n",
    "        stemmer = PorterStemmer()\n",
    "        for tweet in tweets:\n",
    "            words = tweet_token.tokenize(tweet)\n",
    "            words = [w.lower() for w in words]\n",
    "            words = [self.strip_accents(w) for w in words if w not in stopwords.words(\"english\")\n",
    "                     and w.isalpha()\n",
    "                     and w != 'rt'\n",
    "                     and w.find(\"http\") == -1]\n",
    "            words = [lemma.lemmatize(w) for w in words]\n",
    "            list_of_words += words\n",
    "        return list_of_words\n",
    "    \n",
    "    def process_sentences(self,tweets):\n",
    "        list_of_sentences = list()\n",
    "        lemma = WordNetLemmatizer()\n",
    "        tweet_token = TweetTokenizer(strip_handles=True)\n",
    "        stemmer = PorterStemmer()\n",
    "        for tweet in tweets:\n",
    "            words = tweet_token.tokenize(tweet)\n",
    "            words = [w.lower() for w in words]\n",
    "            words = [self.strip_accents(w) for w in words if w not in stopwords.words(\"english\")\n",
    "                     and w.isalpha()\n",
    "                     and w != 'rt']\n",
    "            words = [lemma.lemmatize(w) for w in words]\n",
    "            list_of_sentences.append(words)\n",
    "        return list_of_sentences\n",
    "    \n",
    "    def create_dataframe_figure(self,df):\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        tips = sns.load_dataset(\"tips\")\n",
    "        ax = sns.barplot(x=\"Count\", y=\"Word\", data=df)\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig('bar.png',dpi=400,bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def create_wordcloud(self,strings):\n",
    "        wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(strings)\n",
    "        plt.figure(figsize = (10, 10), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis(\"off\") \n",
    "        plt.tight_layout(pad = 0) \n",
    "        plt.savefig('cloud.png',dpi=400)\n",
    "        plt.show()\n",
    "        \n",
    "    def top_words(self,corpus,number=10):\n",
    "        corpus.sort(key=lambda x: x[1],reverse=True)\n",
    "        return corpus[:number]\n",
    "    \n",
    "    def listen_msg(self,msg):\n",
    "        try:\n",
    "            print(msg['text'])\n",
    "            self.bot.sendMessage(msg['chat']['id'],f'I received: {msg[\"text\"]}')\n",
    "            \n",
    "            self.bot.sendMessage(msg['chat']['id'],'Looking for it...')\n",
    "            entities = self.find_entities(msg['text'])\n",
    "            if(entities == []):\n",
    "                self.bot.sendMessage(msg['chat']['id'],'Please, send another question.')\n",
    "                return 1\n",
    "\n",
    "            person = self.entity_type(entities,'PERSON')\n",
    "            organization = self.entity_type(entities,'ORG')\n",
    "            location = self.entity_type(entities,'GPE')\n",
    "\n",
    "            entities = person + organization + location\n",
    "\n",
    "            string_entities = functools.reduce(lambda x,y: x+'\\n'+y,entities)\n",
    "            self.bot.sendMessage(msg['chat']['id'],f'Entities found:\\n{string_entities}')\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Searching on Twitter...')\n",
    "            query = self.prepare_query(entities)\n",
    "            tweets = tw.Cursor(self.api.search,\n",
    "                  q=query\n",
    "                      ).items(100)\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Processing text from tweets (it may take some time)...')\n",
    "\n",
    "            raw_tweet_text = [x.text for x in tweets]\n",
    "\n",
    "            list_of_words = self.cleaning_tweets(raw_tweet_text)\n",
    "\n",
    "            dataframe_count = pd.DataFrame(Counter(list_of_words) \\\n",
    "                             .most_common(20),columns=['Word','Count'])\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Creating a list with 20 most used words...')\n",
    "            self.create_dataframe_figure(dataframe_count)\n",
    "            self.bot.sendPhoto(msg['chat']['id'],photo=open('bar.png','rb'))\n",
    "            \n",
    "            self.bot.sendMessage(msg['chat']['id'],'Creating a Word Cloud...')\n",
    "            strings = ''\n",
    "            for s in list_of_words:\n",
    "                strings += s+' '\n",
    "            self.create_wordcloud(strings)\n",
    "            self.bot.sendPhoto(msg['chat']['id'],photo=open('cloud.png','rb'))\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Processing most important words...')\n",
    "            list_of_sentences = self.process_sentences(raw_tweet_text)\n",
    "            dct = Dictionary(list_of_sentences)\n",
    "            corpus = [dct.doc2bow(line) for line in list_of_sentences]\n",
    "            tfidf = TfidfModel(corpus)\n",
    "            number_per_corpus = 10\n",
    "            most_important = [self.top_words(tfidf[c],number_per_corpus) for c in corpus]\n",
    "            most_important_names = list()\n",
    "            for m in most_important:\n",
    "                most_important_names += [dct[name[0]] for name in m]\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Creating a list with 10 most important words...')\n",
    "            dataframe_most_important = pd.DataFrame(Counter(most_important_names) \\\n",
    "                             .most_common(10),columns=['Word','Count'])\n",
    "            self.create_dataframe_figure(dataframe_most_important)\n",
    "            self.bot.sendPhoto(msg['chat']['id'],photo=open('bar.png','rb'))\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Creating a Word Cloud with the most important words...')\n",
    "            strings = ''\n",
    "            for s in most_important_names:\n",
    "                strings += s+' '\n",
    "            self.create_wordcloud(strings)\n",
    "            self.bot.sendPhoto(msg['chat']['id'],photo=open('cloud.png','rb'))\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Searching for entities in the tweets...')\n",
    "            sentence_list = list()\n",
    "            for tweet in raw_tweet_text:\n",
    "                sentence_list += sent_tokenize(self.strip_accents(tweet))\n",
    "            entities_tweets = []\n",
    "            for sent in sentence_list:\n",
    "                doc = self.nlp(sent)\n",
    "                for i in doc.ents:\n",
    "                    if(str(i.ents[0]).find('http') == -1):\n",
    "                        entities_tweets.append([i.ents[0], i.ents[0].label_])\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Creating a list with top 10 people mentioned in the tweets...')\n",
    "            person = self.entity_type(entities_tweets,'PERSON')\n",
    "            dataframe_person = pd.DataFrame(Counter(person) \\\n",
    "                         .most_common(20),columns=['Word','Count'])\n",
    "            self.create_dataframe_figure(dataframe_person)\n",
    "            self.bot.sendPhoto(msg['chat']['id'],photo=open('bar.png','rb'))\n",
    "\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Creating a list with top 10 organizations mentioned in the tweets...')\n",
    "            org = self.entity_type(entities_tweets,'ORG')\n",
    "            dataframe_org = pd.DataFrame(Counter(org) \\\n",
    "                         .most_common(20),columns=['Word','Count'])\n",
    "            self.create_dataframe_figure(dataframe_org)\n",
    "            self.bot.sendPhoto(msg['chat']['id'],photo=open('bar.png','rb'))\n",
    "            \n",
    "            self.bot.sendMessage(msg['chat']['id'],'Finished!')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            self.bot.sendMessage(msg['chat']['id'],'Sorry, we had an error! Please, try again')\n",
    "        \n",
    "    def open_twitter(self):\n",
    "        with open('creds.json','r') as file:\n",
    "            credentials = file.read()\n",
    "            json_object = json.loads(credentials)\n",
    "            auth = tw.OAuthHandler(json_object['consumer_key'], \n",
    "                                   json_object['consumer_secret'])\n",
    "            auth.set_access_token(json_object['access_token'], \n",
    "                                  json_object['access_token_secret'])\n",
    "            self.api = tw.API(auth, wait_on_rate_limit=True)\n",
    "        \n",
    "    def start_bot(self):\n",
    "        self.open_twitter()\n",
    "        MessageLoop(self.bot,self.listen_msg).run_as_thread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.start_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
